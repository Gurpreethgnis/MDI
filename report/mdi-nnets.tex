\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{cvpr}
\usepackage{graphicx}
\usepackage{color, colortbl}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption,subcaption}
\usepackage[export]{adjustbox}

% List labels
\usepackage{scrextend}
\addtokomafont{labelinglabel}{\sffamily}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\thefootnote}{$\star$}

\cvprfinalcopy % *** Uncomment this line for the final submission

\usepackage{footmisc} % Daggers for author notes
\DefineFNsymbols{mySymbols}{{\ensuremath\dagger}{\ensuremath\ddagger}\S\P
   *{**}{\ensuremath{\dagger\dagger}}{\ensuremath{\ddagger\ddagger}}}
\setfnsymbol{mySymbols}

% Colors for highlighting tables
\definecolor{Gray}{gray}{0.9}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Missing data imputation for neural networks$^\star$}

\author{
    Jason Poulos\thanks{\href{mailto:poulos@berkeley.edu}{\nolinkurl{poulos@berkeley.edu}}}
    \hspace{10mm}
    Rafael Valle\thanks{\href{mailto:rafaelvalle@berkeley.com}{\nolinkurl{rafaelvalle@berkeley.com}}}
    \vspace{15mm}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Researchers analyzing survey data typically choose decision trees or random forests for prediction tasks, largely because missing data and categorical variables are not easy to handle with neural networks. This paper investigates techniques for handling missing categorical data such that it is appropriate to neural network classifiers. We experiment on two benchmark datasets with missing categorical data, comparing a four-layer neural network with decision tree and random forests classifiers trained on differently imputed data and various levels of perturbation. We beat the state-of-the-art test error on both datasets using a random forests classifier, either trained on data imputed with logistic regression or data with one-hot encoding. We conclude from the results that the performance of the classifiers and imputation strategies generally depend on the nature and proportion of missing data. 
\end{abstract}

\footnotetext[1]{The code used for this project is available on Github (\url{https://github.com/rafaelvalle/MDI}).}

%\linenumbers

%% main text
\section{Introduction} \label{section:Intro}

Missing data is a common problem in survey data in various domains. Random Forests and other decision tree methods are the method of choice for survey data, largely because missing data are not easy to handle with neural networks. We investigate techniques for handling missing categorical data such that it is appropriate to neural network classifiers. We assess the performance of neural network classifiers in comparison with decision tree and random forests classifiers trained on differently imputed data and for various degrees of perturbation.

After briefly reviewing related works in Section~\ref{section:rw}, we experiment on benchmark datasets and compare our results with the state-of-the-art in Section~\ref{section:experiments}. Finally, we draw conclusions in Section~\ref{section:Con}.

\section{Related work}  \label{section:rw}

\subsection{Neural networks for classification with categorical and continuous features}

Common techniques for handling categorical data in
neural networks include encoding the categorical values into numeric values
or using binary encoding. These techniques, however, have some drawbacks
including unnecessarily increasing model complexity or feature dimensionality
and not preserving the similarity information embedded between categorical
values \cite{hsu2006generalizing}. More elaborate techniques include information theoretic measures
\cite{wang2008categorical}, training separate output units for
each of the allowed combination of values of the categorical independent
variables \cite{brouwer2002feed}, and using distance
hierarchies \cite{hsu2006generalizing}. 

In the case of categorical variables, which by definition have no direct
representation or computation scheme of the distance between its values,
decision trees can be useful because they do not require distance metrics.
However, their training process is slow given a large enough dataset and they
might not be suitable for problems where the decision boundary between classes
described by a second-order polynomial, for example \cite{fayyad1996data}.

\subsection{Techniques for handling missing data} \label{section:techniques}
Several techniques for data imputation (i.e., replace missing values with plausible ones) and
direct estimation (i.e., all missing data is analyzed using a maximum likelihood
approach) have been proposed \cite{de2003prevention}. We compare seven different imputation techniques within the following five categories which are commonly found in the previous work on missing data imputation\cite{batista2003analysis}:

\begin{labeling}{MDI}

\item[$k$-nearest-neighbors ($k$-NN)]
Compute the $k$-NN of the observation with missing data and assign the mode of the $k$-neighbors to the missing data.

\item[Classification algorithm]
Train a classification algorithm --- such as logistic regression, random forests or support vector machines (SVMs) --- to classify missing data given existing data. 

%\item[Principal component analysis (PCA)]
%Perform factor analysis (e.g., PCA) on the design matrix, project the design matrix onto the first $N$ eigenvectors and replace the missing values by the values that might be given by the projected design matrix.

\item[Random replacement] 
One observation with missing data is randomly replaced with another non-sampled observation.

\item[Mode replacement]
Replace the missing data with the mode of the corresponding feature vector of the training set.

\item[One-hot] Create a binary variable to indicate whether or not a specific feature is missing.

\end{labeling}

\section{Experiments} \label{section:experiments}

\subsection{Benchmark data sets}

We experiment on two benchmark datasets from the UCI Machine Learning Repository: the Adult dataset and Congressional Voting Records (CVRs) dataset \cite{Lichman2013}. The Adult dataset contains $N=48,842$ observations and 14 features (6 continuous and 8 categorical). Missing values in this dataset are unknown survey responses. The prediction task is to determine whether a person makes over \$50,000 a year. The CVRs dataset contains $N=435$ observations, each the voting record of a member of the $98^{th}$ U.S. House of Representatives for 16 key roll call votes identified by the Congressional Quarterly Almanac. Thus, the dataset contains 16 categorical features with three possible values: ``yea'', ``nay'', and missing. Missing values in this dataset are not simply unknown, but represents values other up-or-down votes, such as voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known. The prediction task is to classify party affiliation (Republican or Democrat). 

We randomly split each dataset $2/3$ for training and $1/3$ for testing. Table \ref{tab:benchmarks} shows the test error rates obtained by the Adult dataset donor \cite{kohavi1996}. The generalization errors were obtained after removing samples with missing values. The state--of--the--art is a Naive Bayes classifier that achieves a  14.05\% error rate. The CVRs dataset donor claims to achieve a 90-95\% accuracy using an incremental decision tree algorithm called STAGGER, although it is not known what train-test split is used or how missing values are handled \cite{schlimmer1987,schlimmer1986}.

\begin{table}[htb]
\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Algorithm}       & \textbf{Error (\%)} \\ \midrule
1  C4.5                  & 15.54          \\
2  C4.5-auto             & 14.46          \\
3  C4.5 rules            & 14.94          \\
4  Voted ID3 (0.6)       & 15.64          \\
5  Voted ID3 (0.8)       & 16.47          \\
6  T2                    & 16.84          \\
7  1R                    & 19.54          \\
8  NBTree                & 14.10          \\
9  CN2                   & 16.00          \\
10 HOODG                 & 14.82          \\
      \rowcolor{Gray}
11 FSS Naive Bayes       & 14.05          \\
12 IDTM (Decision table) & 14.46          \\
13 Naive-Bayes           & 16.12          \\
14 Nearest-neighbor (1)  & 21.42          \\ \bottomrule
\end{tabular}
\caption{Test set error rates on Adult dataset for various algorithms, obtained after removal of samples with missing values and using the original train/test split. Source: \cite{Lichman2013}.}
\label{tab:benchmarks}
   \end{center}
\end{table}

\subsection{Patterns of missing values}

Uncovering patterns of missing values in the dataset will help select strategies for imputing missing values. Figure \ref{fig:proportion-missing-adult} analyzes patterns of missing data in the Adult dataset, in which 7.4\% of the observations contain missing values. Missing values occur in three of the categorical features: \textit{Work class}, \textit{Occupation}, and \textit{Native country}. Since these the former two features are related, it is unlikely that these values are missing completely at random (MCAR); it is more likely, and much less desirable that the values are not missing at random (MNAR). The histogram (left) in Figure \ref{fig:proportion-missing-votes} shows \textit{Work class} and \textit{Occupation} each have about 5.6\% of missing values, and \textit{Native country} has about 1.7\% missing values. The aggregation plot (right) shows 5.5\% of samples are missing values for both \textit{Work class} and \textit{Occupation}. Less than 2\% of samples are missing just \textit{Native country} and less than 1\% are missing all three features.

\begin{figure*}[htbp] 
   \begin{center}
      \textbf{Adult training set ($N=32,561$)}\par\medskip
   \includegraphics[scale=0.5]{figure/proportion-missing-adult.pdf}
  \caption{Histogram of proportion of missing values in each feature (Left) of Adult training set and aggregation plot of all existing combinations of missing and non-missing values in the samples (Right).}
   \label{fig:proportion-missing-adult}
      \end{center}
\end{figure*}

Figure \ref{fig:proportion-missing-votes} shows that 46\% of the CVRs data contains missing values and all features contain missing data. About a quarter of missing data is in \texttt{South Africa}, which was a controversial amendment to amend the Export Administration Act to bar U.S. exports to South Africa's apartheid regime. Twelve percent of missing data is in the feature \texttt{Water}, which is a water projects authorizations bill, and 7\% of missing data rests in the feature \texttt{Exports}, which is a tariff bill. 

\begin{figure*}[htbp] 
   \begin{center}
      \textbf{CVRs training set ($N=291$)}\par\medskip
   \includegraphics[scale=0.5]{figure/proportion-missing-votes.pdf}
  \caption{Histogram of proportion of missing values in each feature (Left) of CVRs training set and aggregation plot of all existing combinations of missing and non-missing values in the samples (Right).}
   \label{fig:proportion-missing-votes}
      \end{center}
\end{figure*}

\subsection{Preprocessing}

The first step in preprocessing the training data is artificially increasing the number of missing values in order to study the effect of larger amounts of missing data. We accomplish this by perturbing the categorical training features by substituting nonmissing values with missing values so that each feature has a minimum missing data ratio, from 0-40\%. For example if there 10 categorical features and 10 observations, a missing data ratio of 0.1 will perturb 10 values. Second, we implement one-hot encoding (-1,1) for the categorical variables. Third, we implement the imputation technique. Lastly, we standardize continuous features by subtracting the mean and dividing by the standard deviation of the feature. We preprocess the test data in the same manner, with the exception that there is no perturbation of test features.\footnote{When imputing the missing data with mode replacement, we use the training set mode. We also use the training set mean and standard deviation to standardize test set features.}

\subsection{Model training and hyperparameter selection} \label{hyper}

We train a four-layer neural network, with each of the two hidden layers having 1024 nodes. We use the adaptive learning rate method Adadelta \cite{zeiler2012} for the update rule. We explore the following hyperparameter space via Bayesian optimization \cite{snoek2012}:

\begin{itemize}
\item Momentum schedule [0 to 1]
\item Dropout regularization [No, Yes]
\item Learning rate: [0.000001 to 0.01].
\end{itemize}

The goal of Bayesian optimization is to choose a point in the hyperparameter space that appropriately balances information gain and exploitation. Figure \ref{fig:params} shows the exploration of hyperparameter space during Bayesian optimization for both Adult and CVRs datasets. Each circle represents a candidate neural network classifier trained on a differently imputed and perturbed dataset. More circles appear in the plot for CVRs simply due to the fact that the training set is smaller. We see that most of the candidate models use dropout and have an initial learning rate close to the maximum of 0.01. The plurality of candidate models appear to either have momentum (1) or not (0). The candidate model with the lowest training error is selected for predicting on the test set. 

\begin{figure*}[htbp] 
   \begin{center}
   \includegraphics[width=1\textwidth,left]{../images/params3d_adult.png}
    \includegraphics[width=1\textwidth,left]{../images/params3d_votes.png}
   \caption{Exploration of hyperparameter space during Bayesian optimization.}
   \label{fig:params}
      \end{center}
\end{figure*}

\subsection{Model assessment}

We assess the performance of the neural network classifier in terms of test set error rate in comparison with decision tree and random forests classifiers on differently imputed data and for various degrees of perturbation.  We use one-hot encoding to represent missing data when no imputation method is used. The results on the Adult dataset and CVRs dataset are plotted in Figures \ref{fig:test-error-adult} and \ref{fig:test-error-votes}, respectively. For the Adult dataset, the random forests classifier trained on data imputed with logistic regression yields the lowest generalization error (13.85\% error), beating the state-of-the-art by 0.2\%. In comparison, random forests trained on data with no missing data imputation matches the state-of-the-art (14.05\%) and neural network trained on data imputed by random replacement performs considerably worse (14.37\%). For the CVRs dataset, random forests trained on one-hot encoded data with up to 20\% of the data perturbed beats the state-of-the-art by over 2\% (2.77\%). Neural networks trained on PCA-imputed data or no imputation also beat the state-of-the-art, with error rates of 3.47\% and 4.16\%, respectively. 

Overall, the performance of the classifiers and imputation strategies depend on the dataset and amount of missing data. For the Adult dataset, random forests classifiers trained on data imputed with other classifiers (i.e., logistic regression, random forests, and SVMs) outperform other classifiers and imputation methods across different ratios of perturbed data. All classifiers trained on one-hot encoded data perform very poorly when the Adult dataset is perturbed. This is likely due to the fact that in the original, non-perturbed Adult dataset, missing values are concentrated in three features which may not be consequential for the prediction task. Perturbation exposes features that are more consequential to the prediction task to missing data. 

Each of the three classifiers trained on one-hot encoded data perform well on the CVRs dataset. In this dataset, missing values represent potentially valuable information for the prediction task and can be more useful for the classifier than the imputed value for certain features. This is why it is not implausible that a classifier trained on perturbed, one-hot encoded data can have lower generalization error than a classifier trained on non-perturbed, one-hot encoded data. 

\begin{figure*}[htbp] 
   \begin{center}
   \includegraphics[scale=0.5]{figure/test-errors-adult-no-imp.pdf}
    \includegraphics[scale=0.5]{figure/test-errors-adult-imp.pdf}
   \caption{Error rates on the Adult test set with (bottom) and without (top) missing data imputation, for various levels of perturbed training features (x-axis). One-hot encoding is used to represent missing data in the absence of imputation. The decision tree and random forests classifiers are trained with maximum depths of 8 and 16, respectively. The structure and hyperparameter selection for the neural network classifier is described in Section \ref{hyper}.}
   \label{fig:test-error-adult}
      \end{center}
\end{figure*}

\begin{figure*}[htbp] 
   \begin{center}
   \includegraphics[scale=0.5]{figure/test-errors-votes-no-imp.pdf}
      \includegraphics[scale=0.5]{figure/test-errors-votes-imp.pdf}
   \caption{Error rates on the CVRs test set with (bottom) and without (top) missing data imputation. See footnotes for Figure \ref{fig:test-error-adult}.}
   \label{fig:test-error-votes}
   \end{center}
\end{figure*}

\section{Conclusion} \label{section:Con}

Neural networks have become a popular machine learning algorithm in many domains, in part due to the ability of neural networks to ``learn'' how to engineer features.  However, researchers analyzing survey data typically choose decision trees or random forests for prediction tasks because missing data and categorical variables are not easy to handle with neural networks. This paper investigates techniques for handling missing data for training neural network classifiers. 

We compare the predictive performance of a four-layer neural network against decision tree and random forest classifiers trained on datasets with differently imputed data. We assess performance in terms of test set error for different levels of perturbed training data, from 0\% (no perturbation) to 40\% perturbation. We beat the state-of-the-art test error on the Adult dataset by 0.2\% using a random forests classifier trained on data imputed with logistic regression. Random forests trained on perturbed and one-hot encoded data outperforms the state-of-the-art on the CVRs dataset by over 2\%. 

We conclude from the results that the performance of the classifiers and imputation strategies generally depend on the nature and proportion of missing data. For the Adult dataset, random forests classifiers trained on data imputed with other classifiers outperform other classifiers and imputation methods across different ratios of perturbed data, while classifiers trained on one-hot encoded data perform very poorly on perturbed training data. This finding can be explained by the fact that missing values in the Adult dataset are concentrated in three features which may not be consequential for the prediction task, and perturbation exposes features that are more consequential to the prediction task to missing data. For the CVRs dataset, each of the three classifiers trained on one-hot encoded data perform well across different levels of perturbation. This finding can be explained by the fact that missing values represent potentially valuable information for the prediction task in the CVRs data.

For future work, we will further explore the idea that missing data may have more predictive value than imputed data in certain domains. A related question is whether neural networks can outperform decision trees or random forests trained on one-hot encoded data by learning different types of missing values. For instance, can neural networks engineer features that reflect the different types of missing values in the CVRs data, where missing values represent any action other than an up-or-down vote (e.g., voted present)? Answers to these questions will help guide researchers in choosing which imputation strategies and classifiers to use for prediction problems in different domains. 

%\section*{Acknowledgments}

{\small
\bibliographystyle{ieee}
\bibliography{refs}
}

\end{document}
